{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4c69ea-8293-4aa8-9888-94a43d075c84",
   "metadata": {},
   "source": [
    "# **Part 1: Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a0723-3872-4178-82ed-f244ecd02fba",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c9052-628e-4299-9050-a9fa19791aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60774d-e5f8-4943-872f-6dcd2cdb61a8",
   "metadata": {},
   "source": [
    "### **Settings**\n",
    "I usually don't work with hardcoded settings like this, but it was the easiest way to create a tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2fb3de-659e-4094-8345-ac9828f23c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation\n",
    "NUMBER_OF_PIXELS = 150\n",
    "START_INDEX_LEFT = 112\n",
    "START_INDEX_RIGHT = 497\n",
    "\n",
    "# Training\n",
    "IMAGE_WIDTH = 128\n",
    "IMAGE_HEIGHT = 64\n",
    "LABEL_PADDING = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c6656-6ae3-4e41-8586-47498767aa6e",
   "metadata": {},
   "source": [
    "# **Part 2: Data Annotation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef69e0-83cb-4b04-9615-f738057c19c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open(\"rail_locations.json\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4fe5a-b33d-4226-bdc5-647c4b686c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rails(image, start_indexes, number_of_pixels):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
    "\n",
    "    for i in range(2):\n",
    "        axes[i].imshow(image.crop([start_indexes[i], 0, start_indexes[i] + number_of_pixels, image.size[1]]), cmap=\"gray\")\n",
    "        axes[i].set_xticks([i for i in range(0, number_of_pixels, 5)])\n",
    "        axes[i].set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130e816-3726-4cfa-b996-f25bd68b4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_rail_images(start_indexes, number_of_pixels, split):\n",
    "    training_data = load_data()\n",
    "    np.random.shuffle(training_data)\n",
    "    \n",
    "    for index, t in enumerate(training_data):\n",
    "        if \"locations\" in t.keys(): continue\n",
    "        clear_output()\n",
    "    \n",
    "        image = Image.open(t[\"image\"])\n",
    "        show_rails(image, start_indexes, number_of_pixels)\n",
    "        locations = input(\"Rail centers:\").split()\n",
    "        locations = [int(l) + start_indexes[i] for i, l in enumerate(locations)]\n",
    "    \n",
    "        training_data[index][\"locations\"] = locations\n",
    "        training_data[index][\"split\"] = split\n",
    "\n",
    "        with open(\"rail_locations.json\", \"w\") as f:\n",
    "            json.dump(training_data, f)\n",
    "\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d4634-7540-435a-9279-77c6d780458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\" # or \"valid\"\n",
    "\n",
    "annotate_rail_images([START_INDEX_L, START_INDEX_R], NUMBER_OF_PIXELS, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa523831-cd71-4eea-b8d9-bcb50af544c3",
   "metadata": {},
   "source": [
    "# **Part 3: Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4013a4-b977-4574-b3f8-792c12e1ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(x, size, padding):\n",
    "    label = torch.zeros(size)\n",
    "    start = max(0, int(round(size * x)) - padding)\n",
    "    end = min(size - 1, int(round(size * x)) + padding + 1)\n",
    "    label[start:end] = 1\n",
    "    return label\n",
    "\n",
    "def flip_image(crop, x):\n",
    "    if np.random.rand() < 0.5:\n",
    "        crop = crop.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        x = 1 - x\n",
    "        \n",
    "    if np.random.rand() < 0.5:\n",
    "        crop = crop.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "def create_crop(image, x, start_index, train, number_of_pixels, size, padding):\n",
    "    x_start = x - (np.random.uniform(10, number_of_pixels - 10)) if train else start_index\n",
    "    crop = image.crop([x_start, 0, x_start + number_of_pixels, image.size[1]])\n",
    "    crop = crop.resize((size, 64), Image.LANCZOS)\n",
    "    x = (x - x_start) / number_of_pixels\n",
    "\n",
    "    if train:\n",
    "        flip_image(crop, x)\n",
    "        crop = TF.adjust_brightness(crop, np.random.uniform(0.5, 1.5))\n",
    "        crop = TF.adjust_contrast(crop, np.random.uniform(0.8, 1.2))\n",
    "    \n",
    "    label = create_label(x, size, padding)\n",
    "\n",
    "    return TF.to_tensor(crop), label\n",
    "\n",
    "def get_crops(data_point, train=False):\n",
    "    left_image, x_left = create_crop(\n",
    "        image=data_point[\"image\"], x=data_point[\"locations\"][0], start_index=START_INDEX_LEFT, \n",
    "        train=train, number_of_pixels=NUMBER_OF_PIXELS, size=IMAGE_WIDTH, padding=LABEL_PADDING\n",
    "    )\n",
    "    \n",
    "    right_image, x_right = create_crop(\n",
    "        image=data_point[\"image\"], x=data_point[\"locations\"][1], start_index=START_INDEX_RIGHT,\n",
    "        train=train, number_of_pixels=NUMBER_OF_PIXELS, size=IMAGE_WIDTH, padding=LABEL_PADDING\n",
    "    )\n",
    "    \n",
    "    return torch.cat([left_image, right_image]), torch.cat([x_left, x_right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0f6d6-3000-4141-8072-d5784f35e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RailDataset(Dataset):\n",
    "    def __init__(self, images, train):\n",
    "        self.images = images\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        images, labels = get_crops(self.images[index], self.train)\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aedc89-8c0c-4292-90c3-722ddad49173",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rail_locations.json\") as f:\n",
    "    data = json.load(f)\n",
    "    data = [d for d in data if \"locations\" in d.keys()]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i][\"image\"] = Image.open(data[i][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638220b-ca68-439b-888f-ef96ba372215",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [d for d in data if d[\"split\"] == \"train\"]\n",
    "data_valid = [d for d in data if d[\"split\"] == \"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096299e-9965-4285-8e66-255f4a846b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "dataset_train = RailDataset(images=data_train, train=True)\n",
    "dataloader_train = DataLoader(dataset_train, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "dataset_valid = RailDataset(images=data_valid, train=False)\n",
    "dataloader_valid = DataLoader(dataset_valid, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc77a2-4484-4e4e-b8d3-ed853c23f080",
   "metadata": {},
   "source": [
    "# **Part 4: Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cbe160-bcc6-4d45-90ed-588cb1b09b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RailDetector(torch.nn.Module):\n",
    "    def __init__(self, size=1):\n",
    "        super(RailDetector, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, size, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(size, 2 * size, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(2 * size, 4 * size, kernel_size=3, stride=2, padding=1)\n",
    "        self.embedding = torch.nn.Linear(4 * size * 2, IMAGE_WIDTH)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), (2, 2)))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), (2, 2)))\n",
    "        return self.embedding(x.view(x.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26df087-8841-4c58-a609-07476295faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, loss_f):\n",
    "    model.eval()\n",
    "    valid_losses = []\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = images.view(-1, IMAGE_HEIGHT, IMAGE_WIDTH).unsqueeze(1)\n",
    "        labels = labels.view(-1, IMAGE_WIDTH)\n",
    "        output = model(images)\n",
    "        loss = loss_f(output, labels)\n",
    "        valid_losses.append(loss.item())\n",
    "    return np.mean(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a43841-7c95-410b-9aef-d86a0d52430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_position(model, data, train=False):\n",
    "    differences = []\n",
    "    \n",
    "    for d in data:\n",
    "        images, labels = get_crops(d, train=train)\n",
    "        images = images.view(-1, IMAGE_HEIGHT, IMAGE_WIDTH).unsqueeze(1)\n",
    "        labels = labels.view(-1, IMAGE_WIDTH)\n",
    "        output = model(images)\n",
    "    \n",
    "        for i in range(2):\n",
    "            x = pd.Series(labels[i].detach().numpy()).rolling(5, center=True, min_periods=1).mean().argmax()\n",
    "            p = pd.Series(output[i].detach().numpy()).rolling(5, center=True, min_periods=1).mean().argmax()\n",
    "            differences.append(abs(x-p))\n",
    "\n",
    "    return np.mean(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad118b4a-09a2-472b-afb2-72b7f5448515",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 5000\n",
    "\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "weights = \"model.pth\"\n",
    "model = RailDetector(size=8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "best_score = get_average_position(model, data_valid)\n",
    "progress_bar = trange(number_of_epochs)\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    train_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        images = images.view(-1, IMAGE_HEIGHT, IMAGE_WIDTH).unsqueeze(1)\n",
    "        labels = labels.view(-1, IMAGE_WIDTH)\n",
    "        output = model(images)\n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    valid_loss = validate(model, dataloader_valid, loss_function)\n",
    "    average_position = get_average_position(model, data_valid)\n",
    "    if average_position < best_score:\n",
    "        best_score = average_position\n",
    "        torch.save(model.state_dict(), weights)\n",
    "\n",
    "    progress_bar.set_description(\"T: {:.3f}, V: {:.3f}, P: {:.2f}, B: {:.2f}\".format(\n",
    "        np.mean(train_losses), valid_loss, average_position, best_score\n",
    "    ))\n",
    "\n",
    "model.load_state_dict(torch.load(weights, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99773bd-5ac6-4432-adca-ceeb2b81edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_red_alpha(l_image):\n",
    "    red_channel = l_image\n",
    "    green_channel = Image.new(\"L\", l_image.size, 0)\n",
    "    blue_channel = Image.new(\"L\", l_image.size, 0)\n",
    "    alpha_channel = Image.new(\"L\", l_image.size, 128)\n",
    "    rgba_image = Image.merge(\"RGB\", (red_channel, green_channel, blue_channel))\n",
    "    return rgba_image, red_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1cbf53-de32-4e6c-bce0-f0b98b746ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blue_alpha(l_image):\n",
    "    blue_channel = l_image\n",
    "    green_channel = Image.new(\"L\", l_image.size, 0)\n",
    "    red_channel = Image.new(\"L\", l_image.size, 0)\n",
    "    alpha_channel = Image.new(\"L\", l_image.size, 128)\n",
    "    rgba_image = Image.merge(\"RGB\", (red_channel, green_channel, blue_channel))\n",
    "    return rgba_image, blue_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4c2ba-da12-41f6-9ec1-50ab3f012d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data_valid:\n",
    "    images, labels = get_crops(d, train=False)\n",
    "    images = images.view(-1, IMAGE_HEIGHT, IMAGE_WIDTH).unsqueeze(1)\n",
    "    labels = labels.view(-1, IMAGE_WIDTH)\n",
    "    output = model(images)\n",
    "\n",
    "    for i in range(2):\n",
    "        red_alpha = labels[i].unsqueeze(0).repeat(IMAGE_HEIGHT, 1)\n",
    "        red_overlay, red_alpha = create_red_alpha(TF.to_pil_image(red_alpha))\n",
    "\n",
    "        blue_alpha = torch.zeros(IMAGE_WIDTH)\n",
    "        x = pd.Series(output[i].detach().numpy()).rolling(5, center=True, min_periods=1).mean().argmax()\n",
    "        for j in range(-2, 3):\n",
    "            if x+j >= 0 and x+j < IMAGE_WIDTH:\n",
    "                blue_alpha[x+j] = 1\n",
    "        \n",
    "        blue_alpha = blue_alpha.unsqueeze(0).repeat(IMAGE_HEIGHT, 1)\n",
    "        blue_overlay, blue_alpha = create_blue_alpha(TF.to_pil_image(blue_alpha))\n",
    "        \n",
    "        pil_image = TF.to_pil_image(images[i]).convert(\"RGBA\")\n",
    "        pil_image.paste(red_overlay, (0, 0), mask=Image.fromarray((np.array(red_alpha) / 2).astype(np.uint8)))\n",
    "        pil_image.paste(blue_overlay, (0, 0), mask=Image.fromarray((np.array(blue_alpha) / 2).astype(np.uint8)))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(pil_image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13453f3a-3002-4715-ac00-62313f86a6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
